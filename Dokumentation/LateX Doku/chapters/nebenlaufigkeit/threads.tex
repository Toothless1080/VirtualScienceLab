% !TeX root = ../../pythonTutorial.tex
\subsection{Threads}
\label{threads:subsection:threads}

In Python werden zwei APIs zur Verwendung von Threads angeboten, die Low-Level
API aus dem \_thread-Modul und die Higher-Level API aud dem threading-Modul.
Es wird sich an dieser Stelle auf das threading-Modul beschränkt, da es intern auf
dem \_thread-Modul basiert und eine Schnittstelle anbietet, die das Programmieren
von Multithreaded-Programmen erleichert.
Diese Schnittstelle ist an der Thread-Schnittstelle von Java angelehnt und sollte daher für
Java-Entwickler leicht zu verwenden sein.
Allerdings gibt es einige Unterschiede zwischen dem Python-Modul und der entsprechenden
Implementierung in Java.
So sind Bedingungsvariablen und Locks seperate Objekte in Python und es ist auch
nur eine Teilmenge des Verhaltens eines Java-Threads in Python verfügbar.
Ein Python-Thread kennt keine Prioritäten und Thread-Gruppen und er kann nicht zerstört,
gestopped, angehalten, fortgesetzt oder unterbrochen werden.
Soweit vorhanden sind die statischen Methoden aus der Java-Thread-Klasse auf Modul-Ebene
in Python implementiert.

\subsubsection{Thread Objekte}
\label{threads:subsubsection:thread_objekte}

Es gibt zwei Möglichkeiten einen Thread zu erzeugen. Entweder wird dem Konstruktor ein
aufrufbares Objekt übergeben,

\label{threads:lst:thread_erzeugung_callable}
\lstinputlisting[language=Python,linerange={1-2,6-12}]{chapters/nebenlaufigkeit/src/thread_erzeugung.py}

oder die \lstinline$run()$-Methode wird in einer von \lstinline$Thread$ abgeleiteten Klasse überschrieben.

\label{threads:lst:thread_erzeugung_subclass}
\lstinputlisting[language=Python,linerange={1-2,14-21}]{chapters/nebenlaufigkeit/src/thread_erzeugung.py}

Der Konstruktor  der \lstinline$Thread$-Klasse bietet noch weitere Parameter an:
\begin{itemize}
    \item \lstinline$group$ sollte immer \lstinline$None$ sein.
    Es ist aktuell reserviert für spätere Erweiterungen.
    \item \lstinline$name$ setzt den Namen des Threads.
    \item \lstinline$args$ ist ein Tupel aus Parametern für das mit \lstinline$target$ definierte
    aufrufbare Objekt.
    \item \lstinline$kwargs$ ist ein Dictionary aus Schlüsselwort-Parametern für \lstinline$target$.
    \item \lstinline$deamon$ setzt die Dämon-Eigenschaft des Threads.
\end{itemize}

Es ist anzumerken, dass ein \lstinline$Thread$-Objekt bei seiner Erzeugung noch nicht gestartet wird.
Hierzu muss explizit die \lstinline$start()$-Methode aufgerufen werden.
Wurde ein Thread gestartet, wird er als \glqq lebendig\grqq{} angesehen. 
Dies bleibt er solange, bis seine \lstinline$run()$-Methode verlassen wurde.
Hierbei macht es keinen Unterschied, ob sie regulär verlassen wurde oder Aufgrund einer Exception.
Der aktuelle Status eines Threads kann mittels der \lstinline$is_alive()$-Methode abgefragt werden.
Soll auf das Beenden eines anderen Threads gewartet werden, so kann seine
\lstinline$join()$-Methode aufgerufen werden. 
Hiermit wird der aufrufende Thread blockiert, bis der andere beendet ist.
Die \lstinline$join$-Methode nimmt einen optionalen Parameter des Typen \lstinline$float$ entgegen,
der als Timeout in Sekunden dient.
Wird ein Timeout angegeben, ist es wichtig, dass nach dem \lstinline$join()$-Aufruf die Methode 
\lstinline$is_alive()$ aufgerufen wird.
Da \lstinline$join()$ immer \lstinline$None$ zurückgibt, ist es ansonsten nicht möglich, zu wissen, ob
der Thread tatsächlich beendet wurde, oder nur der Timeout abgelaufen ist.

\kontrollfrage{
\item[\kontroll] Wie kann auf das Ende der Ausführung eines Threads gewartet werden?
}

Jeder Thread besitzt einen Namen, der initial über den Konstruktor oder direkt über das
\lstinline$name$-Attribut gesetzt werden kann. 
Threads können als Dämon gekennzeichnet werden.
Sobald nur noch Dämon-Threads aktiv sind, wird das Python-Programm beendet.
Die Dämon-Eigenschaft kann initial über den Konstruktor gesetzt werden.
Wird kein Wert übergeben, übernimmt der Thread standardmäßig den Wert des erzeugenden Threads.
Über das \lstinline$deamon$-Attribut eines Threads kann die Eigenschaft abgefragt und gesetzt werden.
Hierbei ist es wichtig, dass die Eigenschaft immer vor dem Aufruf der \lstinline$start()$-Methode
gesetzt wird.
Wird sie nach dem Starten des Threads geändert, so wird ein \lstinline$RuntimeError$ geworfen.

\warning{
	Dämon-Threads werden sofort beendet, wenn keine normalen Threads mehr aktiv sind.
	Das heißt, dass ihre Ressourcen, wie zum Beispiel geöffnete Dateien oder Datenbanktransaktionen,
	gegebenenfalls nicht ordentlich freigegeben werden.
	Um dies zu verhindern, sollten die betroffenen Threads nicht die Dämon-Eigenschaft besitzen und
	es sollten geeignete Signalisierungsmechanismen eingesetzt werden (siehe Abschnitt
	\ref{threads:subsubsection:thread-kommunikation}).
}

\uebung
\aufgabe{nebenlaufigkeit01}
\aufgabe{nebenlaufigkeit02}


\subsubsection{Synchronisation}
\label{threads:subsubsection:synchronisation}

Die meisten Anwendungen, in denen mehrere Threads zum Einsatz kommen, erfordern einen
Mechanismus, der die Zugriffe der einzelnen Threads auf gewisse Daten synchronisiert.
Hierdurch wird unteranderem vermieden, dass auf invaliden Datensätzen gearbeitet wird, oder ein
Datenupdate verloren geht.
Im Folgenden wird ein Beispiel betrachtet, bei dem es zu Fehlern aufgrund von fehlender
Synchronisationsmechanismen kommt.
Es werden anschließend neue Konstrukte eingeführt, die die Fehler beheben werden.

Betrachtet wird nun die folgende \hyperref[threads:lst:counter_example]{\lstinline$Counter$-Klasse}.
Sie besitzt das Attribut \lstinline$count$, welches durch Aufruf von \lstinline$increment()$
in Einerschritten erhöht wird. 

\label{threads:lst:counter_example}
\lstinputlisting[language=Python,linerange={1-3,7-13}]{chapters/nebenlaufigkeit/src/synchronisation_fehler.py}

Es ist weiterhin die \hyperref[threads:lst:incrementer_thread]{\lstinline$IncrementerThread$-Klasse}
gegeben, welche bei der Initialisierung ein \lstinline$Counter$-Objekt erwartet.
Dieser Thread ruft eine Millionen mal die \lstinline$increment()$-Methode des \lstinline$Counters$
auf und beendet sich anschließend.

\label{threads:lst:incrementer_thread}
\lstinputlisting[language=Python,linerange={1-3,15-23}]{chapters/nebenlaufigkeit/src/synchronisation_fehler.py}

Für dieses Beispiel werden nun 10 \lstinline$IncrementerThreads$ erzeugt und gestartet.
Anschließend wird auf ihre Terminierung gewartet und dann der Wert des \lstinline$count$-Attributs des
\lstinline$Counters$ ausgegeben:

\label{threads:lst:example_no_locks}
\lstinputlisting[language=Python,linerange={1-3,25-36}]{chapters/nebenlaufigkeit/src/synchronisation_fehler.py}

\kontrollfrage{
\item[\kontroll] Welche Ausgabe würde erwartet werden, wenn 10 Threads den Counter eine Millionen
mal inkrementieren?
}

Dieser Programmcode würde vermuten lassen, dass bei jeder Ausführung der Wert 10000000 ausgegeben
wird, da jeder der 10 Threads den \lstinline$Counter$ eine Millionen mal inkrementiert.
Erstaunlicherweise werden allerdings bei mehrmaliger Ausführung unterschiedliche Werte ausgegeben.
Diese können zum Beispiel wie folgt aussehen:

\label{threads:lst:beispiel_ausgabe}
\begin{lstlisting}
# Mögliche Ausgaben:
5237496
3561559
4089438
4526494
\end{lstlisting}

Dieses Phänomen lässt sich so erklären, dass die Operation in \lstinline$increment()$ nicht atomar ist.
Genau genommen werden in ihr drei Operationen, eine lesende, eine addierende und eine schreibende,
ausgeführt.
Somit kann es vorkommen, dass zum Beispiel der erste Thread den aktuellen \lstinline$count$-Wert liest
und dann die aktive Ausführung an einen anderen Thread abgeben muss.
Dieser zweite Thread liest nun den selben \lstinline$count$-Wert wie der erste Thread, inkrementiert ihn
und schreibt den neuen Wert zurück in das Attribut.
Nun wechselt die aktive Ausführung zurück zum ersten Thread, welcher noch den alten
\lstinline$count$-Wert gelesen hat.
Dieser alte Wert wird nun erneut inkrementiert und zurückgeschrieben. 
Somit wurde der \lstinline$Counter$ effektiv nicht zweimal, sondern nur einmal inkrementiert.
Um dieses Verhalten zu verhindern, muss sichergestellt werden, dass die drei einzelnen Operationen
atomar ausgeführt werden, sprich, dass sie entweder ganz oder gar nicht ausgeführt werden.

Als unterste Synchronisationsebene bietet Python die Klasse \lstinline$Lock$ an.
\randnotiz{Locks}
Ein Objekt dieser Klasse befindet sich immer in einem von zwei Zuständen, es ist entweder offen
oder geschlossen.
Nach der Initialisierung befindet es sich zuerst im geöffneten Zustand.
Ein \lstinline$Lock$-Objekt stellt die beiden Methoden \lstinline$acquire()$ und \lstinline$release()$ zur
Verfügung.
Wird \lstinline$acquire()$ auf einem offenen \lstinline$Lock$ aufgerufen, so begibt sich das \lstinline$Lock$
in den geschlossenen Zustand und die Methode kehrt sofort zurück.
Sollte die \lstinline$aquire()$-Methode aufgerufen werden, wenn sich das \lstinline$Lock$ im
geschlossenen Zustand befindet, so blockiert sie solange, bis \lstinline$release()$ in einem anderen Thread
aufgerufen wird und somit den Zustand des \lstinline$Locks$ wieder zu geöffnet ändert.
Die blockierte \lstinline$aquire()$-Methode schließt dann das \lstinline$Lock$ wieder und kehrt zurück.
Wird auf einem offenen \lstinline$Lock$ die \lstinline$release()$-Methode aufgerufen, so wird ein
\lstinline$RuntimeError$ geworfen.
Falls mehrere Threads durch \lstinline$aquire()$ blockiert werden, wird nur ein Thread fortgesetzt, sobald
\lstinline$release()$ aufgerufen wurde.
Welcher der blockierten Threads fortgesetzt wird, ist hierbei nicht definiert.

Wurde \lstinline$aquire()$ aufgerufen, sollte garantiert sein, dass auch \lstinline$release()$
aufgerufen wird.
Wird eine \lstinline$Exception$ geworfen, kann dies allerdings nicht immer garantiert sein.
Aus diesem Grund wird empfohlen, den durch das \lstinline$Lock$ geschützten Programmcode in
einen \lstinline$try$-Block zu schreiben und den Aufruf von \lstinline$release()$ in den
\lstinline$finally$-Block zu schreiben:

\label{threads:lst:try_aquire_lock}
\lstinputlisting[language=Python,linerange={1-2,7-12}]{chapters/nebenlaufigkeit/src/lock_aquire_release.py}

Das Gleiche kann mit dem \lstinline$with$-Statement erreicht werden, da die \lstinline$Lock$-Klasse
das Context-Management-Protokoll unterstützt.
Hierbei werden die beiden Methoden \lstinline$aquire()$ und \lstinline$release()$ automatisch aufgerufen:

\label{threads:lst:with_aquire_lock}
\lstinputlisting[language=Python,linerange={1-2,13-15}]{chapters/nebenlaufigkeit/src/lock_aquire_release.py}

Ist es nicht gewünscht, dass \lstinline$aquire()$ blockiert, wenn sie mehrmals aufgerufen wird, ohne 
dass das \lstinline$Lock$ freigegeben wurde, so kann ihr auch der optionale Parameter
\lstinline$blocking=False$ übergegeben werden. 
In diesem Fall kehrt \lstinline$aquire()$ sofort zurück, egal in welchem Zustand sich das \lstinline$Lock$
befindet.
Es muss nun der Rückgabewert von \lstinline$aquire()$ betrachtet werden, um zu erfahren, ob das 
\lstinline$Lock$ offen oder geschlossen ist.
Ist das \lstinline$Lock$ bereits geschlossen, wird der Wert \lstinline$False$ zurückgegeben.
Andernfalls wird \lstinline$True$ zurückgegeben und das \lstinline$Lock$ ändert seinen Zustand zu
geschlossen.
Weiterhin ist es möglich, einen Timeout mittels des optionalen Parametes \lstinline$timeout$ zu 
spezifizieren.
Hierbei kann eine beliebige Zeit in Sekunden als \lstinline$float$ Wert angegeben werden.
In diesem Fall blockiert \lstinline$aquire()$ maximale die spezifizierte Zeit.
Wurde in dieser Zeit das \lstinline$Lock$ erlangt, so gibt \lstinline$aquire()$ \lstinline$True$ zurück,
andernfalls \lstinline$False$.
Die Angabe eines Timeouts ist nur erlaubt, wenn der Parameter \lstinline$blocking$ den Wert
\lstinline$True$ besitzt.

\uebung
\aufgabe{nebenlaufigkeit03}
\aufgabe{nebenlaufigkeit04}

Um das Problem aus Aufgabe \ref{nebenlaufigkeit04} zu lösen, bietet Python eine weitere Möglichkeit
zur Synchronisation an.
\randnotiz{Reentrant Locks}
Hierbei handelt es sich um die \lstinline$RLock$-Klasse.
Das \lstinline$R$ steht für \lstinline$reentrant$, was Wiedereintritt bedeutet.
Im Gegensatz zu Objekten der \lstinline$Lock$-Klasse, die nie einem Thread zugeordnet werden, werden
Objekte der \lstinline$RLock$-Klasse an den Thread gebunden, der zuerst die \lstinline$aquire()$-Methode
aufruft.
Neben den beiden Zuständen, die die \lstinline$Lock$-Klasse besitzt, merkt sich die \lstinline$RLock$-Klasse
nun auch, wie oft die \lstinline$aquire()$-Methode aufgerufen wurde.
Beim ersten Aufruf von \lstinline$aquire()$ merkt sich das \lstinline$RLock$-Objekt, welcher Thread die
Methode aufgerufen hat und setzt einen internen Zähler auf 1.
Bei jedem weiteren Aufruf von \lstinline$aquire()$ desselben Threads wird der Zähler inkrementiert.
Wird \lstinline$release()$ aufgerufen, so wird der Zähler wieder dekrementiert.
Das \lstinline$RLock$ ist erst dann wieder offen, wenn die Methode \lstinline$release()$ genau so oft
aufgerufen wurde, wie \lstinline$aquire()$, und der interne Zähler wieder auf 0 steht.
Ruft ein zweiter Thread die \lstinline$aquire()$-Methode auf, während der erste Thread das
\lstinline$RLock$ besitzt, so muss er warten, bis der Zähler wieder auf 0 steht.
Es ist nun also möglich, einen gewissen Codeabschnitt auch rekursiv vor konkurrierenden Zugriffen 
zu schützen.
Wie auch schon beim \lstinline$Lock$, können der \lstinline$aquire()$-Methode des \lstinline$RLocks$ 
die beiden optionalen Paramter \lstinline$blocking$ und \lstinline$timeout$ übergegeben werden.

\uebung
\aufgabe{nebenlaufigkeit05}

Im Folgenden wird das Beispiel mit dem \lstinline$Counter$ und dem \lstinline$Incrementer-$
\lstinline$Thread$ etwas angepasst.
Der \lstinline$IncrementerThread$ soll nun den \lstinline$Counter$ wieder nur um eins erhöhen.
Weiterhin wird dem \lstinline$IncrementerThread$ ein Wert übergeben, mit dem gesteuert wird,
wann der Thread den \lstinline$Counter$ erhöht. Den 10 erstellten \lstinline$IncrementerThreads$ 
wird nun eine Zahl von 0 bis 9 übergeben. Der \lstinline$Counter$ soll von den einzelnen Threads immer
nur dann erhöht werden, wenn der aktuelle Wert des \lstinline$Counters$ auf die Ziffer endet, die dem
Thread bei der Erzeugung übergeben wurde.
Demnach müssen die \lstinline$IncrementerThreads$ auf einen bestimmten geteilten Zustand warten,
bevor sie \lstinline$increment()$ aufrufen dürfen.
\randnotiz{Condition-Variable}
Für einen solchen Anwendungsfall stellt Python die \lstinline$Condition$-Klasse zur Verfügung.
Der Mechanismus, der hierdurch implementiert wird, ist allgemein als Condition Variable
(deutch Bedingungsvariable) bekannt.
Objekte der \lstinline$Condition$-Klasse sind immer einem \lstinline$Lock$- oder einem
\lstinline$RLock$-Objekt zugeordnet.
Dieses kann dem Konstruktor eines \lstinline$Condition$-Objekts übergeben werden.
Wird kein Lock-Objekt übergeben, erzeugt der Konstruktor ein neues.
Das so erzeugte \lstinline$Condition$-Objekt kann nun überall wie das Lock-Objekt
verwendet werden, das Lock-Objekt muss nicht weiterhin zusätzlich verwaltet werden.
Es kann nun also das \lstinline$RLock$ aus der \lstinline$Counter$-Klasse gegen eine Condition Variable
ausgetauscht werden.
Die neue \hyperref[threads:lst:counter_condition_variable_example]{\lstinline$Counter$-Klasse} sieht
dann wie folgt aus:

\label{threads:lst:counter_condition_variable_example}
\lstinputlisting[language=Python,linerange={1-2,6-18}]{chapters/nebenlaufigkeit/src/condition_variable.py}

Eine Condition Variable kann also wie ein einfaches Lock verwendet werden.
Die \lstinline$aquire()$- und \lstinline$release()$-Methoden verhalten sich hierbei wie die des hinterlegten
Lock-Objekts.
Darüber hinaus bietet die \lstinline$Condition$-Klasse noch weitere Methoden an.
Diese Methoden dürfen nur aufgerufen werden, wenn zuvor \lstinline$aquire()$ aufgerufen wurde.
Wurde von einem Thread \lstinline$aquire()$ aufgerufen, aber der aktuelle geteilte Zustand
nicht den gewünschten Bedingungen entspricht, so wird \lstinline$wait()$ aufgerufen.
Die \lstinline$wait()$-Methode gibt das Lock wieder frei und blockiert den Thread, bis er
aufgeweckt wird.
Ein Thread wird durch Aufruf der \lstinline$notify()$- oder der \lstinline$notifiy_all()$
-Methode aufgeweckt.
Diese Methoden sollten immer dann aufgerufen werden, wenn der geteilte Zustand von einem Thread 
geändert wurde.
Sobald ein Thread aufgeweckt wurde, fordert \lstinline$wait()$ wieder das Schloss an und kehrt
dann zurück.
Nachdem \lstinline$wait()$ zurückgekehrt ist, sollten die Bedingungen an den geteilten Zustand auf 
jeden Fall wieder geprüft werden, da eine unbestimmte Zeit zwischen dem Aufruf von \lstinline$notify()$
oder \lstinline$notify_all()$ und dem Zurückkehren von \lstinline$wait()$ vergehen kann.
Weiterhin ist es möglich, \lstinline$wait()$ den optionalen Parameter \lstinline$timeout$ mitzugeben.
Läuft diese Zeit ab, bevor ein anderer Thread \lstinline$notify()$ oder \lstinline$notify_all()$ aufruft,
kehrt \lstinline$wait()$ mit dem Rückgabewert \lstinline$False$ zurück.

\tip{
Um die Entscheidung zwischen \lstinline$notify()$ und \lstinline$notify_all()$ zu erleichtern, sollte die Frage
gestellt werden, ob die Änderung des geteilten Zustands für nur einen Thread oder mehrere Threads
interessant ist.
}

Durch einen Aufruf von \lstinline$notify_all()$ werden alle Threads, die \lstinline$wait()$ auf dem
entsprechenden \lstinline$Condition$-Objekt aufgerufen haben, aufgeweckt.
Mit \lstinline$notify()$ wird nur ein Thread aufgeweckt.
Es kann ein optionaler Parameter \lstinline$n$ an \lstinline$notify()$ übergeben werden, der angibt, wie
viele Threads aufgeweckt werden sollen.

\warning{
	Durch den Aufruf von \lstinline$notify()$ und \lstinline$notify_all()$ wird das Lock nicht freigegeben.
	Das heißt, dass Threads, die \lstinline$wait()$ aufgerufen haben, erst dann wieder aufwachen, wenn
	der Thread, der \lstinline$notify()$ oder \lstinline$notify_all()$ aufgerufen hat das Lock wieder 
	explizit frei gibt.
}

\kontrollfrage{
\item[\kontroll] Ist es für das \lstinline$Counter$ und \lstinline$IncrementerThread$ Beispiel ausreichend,
\lstinline$notify()$ aufzurufen?
}

Das \lstinline$Counter$ und \lstinline$IncrementerThread$ Beispiel würde nicht funktionieren, wenn 
immer nur ein beliebiger Thread geweckt wird. 
Wird der \lstinline$Counter$ inkrementiert, muss \lstinline$notify_all()$ aufgerufen werden,
um einen Deadlock zu vermeiden.
Dies liegt daran, das immer nur genau ein wartender Thread fortschreiten kann und es ist nicht
garantiert werden kann, dass genau dieser Thread aufgeweckt wird.

Wie das generische \hyperref[threads:lst:producer_consumer]{Producer-Consumer-Pattern}
mithilfe von Condition Variablen implementiert werden kann, ist im folgenden Listing
gezeigt (vgl. \cite{pythondokuthreads}).
Die Consumer warten so lange, bis der geteilte Zustand der Bedingung entspricht.
In diesem Fall heißt das, dass mindestens ein Element verfügbar ist.
Sobald ein Producer ein Element erstellt hat, ruft er \lstinline$notify()$ auf.
Somit wird genau ein Consumer aufgeweckt.
In diesem Beispiel reicht es vollkommen aus, nur einen Consumer aufzuwecken.

\label{threads:lst:producer_consumer}
\lstinputlisting[language=Python,linerange={1-2,23-33}]{chapters/nebenlaufigkeit/src/producer_consumer.py}

Der Consumer ruft \lstinline$wait()$ innerhalb der \lstinline$while$-Schleife auf und prüft
jedes mal seine Bedingung.
Dies ist notwendig, da sich der Zustand zwischen dem Aufruf von \lstinline$notify()$ und dem 
Zurückkehren von \lstinline$wait()$ erneut ändern kann. 
Diese Problematik ist in der Multithreaded-Programmierung inhärent.
Die \lstinline$Condition$-Klasse bietet neben \lstinline$wait()$ eine weitere Methode an, die das Testen
der Bedingung automatisieren kann.
Bei dieser Methode handelt es sich um \lstinline$wait_for()$.
Ihr Paramater \lstinline$predicate$ nimmt ein Callable-Objekt entgegen, welches einen boolischen Wert
zurück gibt.
Es kann zudem ein Timeout angegeben werden, der sich wie bei \lstinline$wait()$ verhält.
Wird \lstinline$wait_for()$ verwendet, ändert sich das
\hyperref[threads:lst:producer_consumer_wait_for]{Producer-Consumer-Pattern Beispiel}
folgender Maßen (vgl. \cite{pythondokuthreads}):

\label{threads:lst:producer_consumer_wait_for}
\lstinputlisting[language=Python,linerange={1-2,34-38}]{chapters/nebenlaufigkeit/src/producer_consumer.py}

\uebung
\aufgabe{nebenlaufigkeit06}

Eine weitere Problematik, die bei der Multithreaded-Programmierung vorkommt, ist das Schützen
einer Ressource mit einer begrenzten Kapazität.
Als Beispiel aus dem Alltag dient hierfür ein Parkhaus, welches nur eine begrenzte Anzahl an
Parkplätzen besitzt.
Es dürfen sich immer nur maximal so viele Autos im Parkhaus befinden, wie Parkplätze vorhanden sind.
Die Klasse \hyperref[threads:lst:parkhaus]{\lstinline$CarPark$} simuliert ein Parkhaus und verwendet
eine Condition Variable, um sich vor konkurrierenden Zugriffen zu schützen.

\label{threads:lst:parkhaus}
\lstinputlisting[language=Python,linerange={1-2,8-32}]{chapters/nebenlaufigkeit/src/parkhaus.py}

Ein Auto, dass zuerst eine gewisse Zeit umher fährt, bevor es eine zufällige Zeit parkt, wird durch die
Klasse \hyperref[threads:lst:auto]{\lstinline$Car$} simuliert.
Um eine zufällige Zeit im Intervall von \lstinline$x$ bis \lstinline$y$ zu schlafen wird hier die
\lstinline$uniform(x,y)$-Methode aus dem \lstinline$random$-Modul verwendet.

\label{threads:lst:auto}
\lstinputlisting[language=Python,linerange={1-2,34-57}]{chapters/nebenlaufigkeit/src/parkhaus.py}

Es wird nun ein neues \lstinline$CarPark$-Objekt erzeugt, das fünf Parkplätze besitzt.
Anschließend werden zehn \lstinline$Car$-Objekte initialisiert und gestartet.
Wird das folgende \hyperref[threads:lst:parkhaus_ausführung]{Programm} ausgeführt, kann über die
Ausgaben nachvollzogen werden, welches Auto gerade in das Parkhaus einfahren möcht, einen
Parkplatz gefunden hat und wieder ausfährt.

\label{threads:lst:parkhaus_ausführung}
\lstinputlisting[language=Python,linerange={1-2,59-63}]{chapters/nebenlaufigkeit/src/parkhaus.py}

Dieses Beispiel stellt eine vereinfachte Ansicht auf das reale Geschehen.
Es wird hier nämlich keine Rücksicht auf die Reihenfolge genommen, in der die Autos in das 
Parkhaus einfahren möchten.
Demnach ist es möglich, dass Autos in der Warteschlange übersprungen werden.

\kontrollfrage{
\item[\kontroll] Weshalb wird hier die Reihenfolge der Autos in der Warteschlange nicht beachtet?
}

Es ist etwas lästig, selbst über die aktuell belegten Ressourcen Buch zu führen.
\randnotiz{Semaphoren}
Durch die \lstinline$Semaphore$-Klasse wird in Python eine Synchronisationsprimitive angeboten, 
die das Verwalten der zu schützenden Ressourcen übernimmt.
Eine \lstinline$Semaphore$ ähnelt einem \lstinline$Lock$, mit dem Unterschied, dass ihr bei der Erzeugung
über den \lstinline$value$-Parameter eine Kapazität übergeben werden kann.
Intern verwaltet sie einen Zähler, der mit der angegebenen Kapazität initialisiert wird.
Jedes mal, wenn \lstinline$aquire()$ aufgerufen wird, wird dieser Zähler dekrementiert.
Die \lstinline$aquire()$-Methode blockiert nur dann, wenn der Zähler auf 0 steht.
Erst wenn \lstinline$release()$ aufgerufen wird, wird der Zähler wieder inkrementiert.
Werden Threads durch \lstinline$aquire()$ blockiert, wird genau einer dieser Threads automatisch
aufgeweckt, sobald \lstinline$release()$ aufgerufen wird.
Wie auch zuvor bei \lstinline$Lock$ unterstützt die \lstinline$aquire()$-Methode der
\lstinline$Semaphore$-Klasse die beiden optionalen Parameter \lstinline$blocking$ und \lstinline$timeout$.
Hierbei ist zu beachten, dass die \lstinline$Semaphore$-Klasse es zulässt, \lstinline$release()$ öfter 
aufzurufen, als \lstinline$aquire()$.
Dies hat zur Folge, dass der intere Zähler größer anwächst, als er initial gesetzt wurde, und somit die 
maximale Anzahl an gleichzeitigen Zugriffen auf die geschützte Ressource erhöht wird.
Ist ein solches Verhalten nicht gewünscht, bietet Python die \lstinline$BoundedSemaphore$-Klasse an.
Würde hier durch einen Aufruf von \lstinline$release()$ der interne Zähler größer werden, als der initial
gestezte Wert, so wird anstelle des Inkrementierens des Zählers ein \lstinline$ValueError$ geworfen.

\tip{
Um eventuelle Programmierfehler zu reduzieren, sollte \lstinline$Bounded-$ \lstinline$Semaphore$
bevorzugt verwendet werden, da andernfalls der Fehler unbemerkt bleibt.
}

Wie eine \lstinline$Semaphore$ den gleichzeitigen Zugriff auf eine Datenbankverbindung beschränken
kann ist im folgenden Beispiel gezeigt (vgl. \cite{pythondokuthreads}).
Hier wird eine beschränkung von maximal fünf gleichzeitigen Zugriffen gesetzt.
Das \lstinline$with$-Statement ruft wieder automatisch \lstinline$aquire()$ und \lstinline$release()$ auf.
Somit ist garantiert, dass sich immer maximal fünf Threads innerhalb des \lstinline$with$-Statements 
befinden.

\label{threads:lst:semaphore}
\lstinputlisting[language=Python,linerange={1-2,5-18}]{chapters/nebenlaufigkeit/src/semaphore.py}

\uebung
\aufgabe{nebenlaufigkeit07}

In manchen Situationen ist es notwendig, dass bestimmte Threads aufeinander warten, bevor sie mit
ihrer Ausführung fortschreiten.
\randnotiz{Barrier}
So kann es zum Beispiel sein, dass mit der eigentlichen Aufgabe gewartet werden muss, bis gewisse
Initialisierungen geschehen sind.
Es könnte auch ein Algorithmus betrachtet werden, dessen Teilschritte zwar für sich betrachtet
nebenläufig ausgeführt werden können, es aber zu Fehlern kommt, wenn mit Schritt 2 begonnen wird,
bevor Schritt 1 vollständig bearbeitet wurde.
Für solche Anwendungsfälle bietet Python mit der \lstinline$Barrier$-Klasse eine weitere
Synchronisationsprimitive an.
Ihr wird bei der Erzeugung über den \lstinline$parties$-Parameter angegeben, wie viele Threads 
aufeinander warten müssen.
Die einzelnen Threads rufen die \lstinline$wait()$-Methode der \lstinline$Barrier$ auf, welche solange 
blockiert, bis die mit \lstinline$parties$ spezifizierte Anzahl an Threads \lstinline$wait()$ aufgerufen haben.
Wurde diese Anzahl erreicht, werden alle Threads zeitgleich fortgesetzt.
Neben \lstinline$parties$ nimmt der Konstruktor von \lstinline$Barrier$ die beiden optionalen Parameter
\lstinline$timeout$ und \lstinline$action$.
Durch \lstinline$timeout$ wird der Default-Timeout der \lstinline$wait()$-Methode gesetzt.
Läuft dieser Timeout ab, bevor alle Threads an der \lstinline$Barrier$ warten, werden die bereits
wartenden Threads wieder aufgeweckt und die \lstinline$Barrier$ wird in einen \glqq zerstörten\grqq{}
Zustand gebracht.
Der \lstinline$wait()$-Methode kann zudem ebenfalls ein \lstinline$timeout$-Parameter übergeben werden.
In diesem Fall hat der in \lstinline$wait()$ definierte Timeout vorrang.
Durch den \lstinline$action$-Parameter kann ein Callable-Objekt definiert werden, dass aufgerufen wird,
sobald alle Threads \lstinline$wait()$ aufgerufen haben und bevor sie ihre Ausführung fortsetzen.
Der Rückgabewert von \lstinline$wait()$ ist ein Integer im Intervall [0, \lstinline$parties$). 
Hierüber kann zum Beispiel ein Thread ausgewählt werden, um eine besondere Aufgabe auszuführen.
Wurde die \lstinline$Barrier$ genutzt, kann sie über \lstinline$reset()$ in ihren Initialzustand versetzt
werden und erneut verwendet werden.
Durch Aufruf von \lstinline$abort()$ wird die \lstinline$Barrier$ in einen \glqq zerstörten\grqq{}
Zustand überführt.
Es kann jederzeit die Anzahl der Threads, auf die gewartet wird, und die Anzahl der bereits wartenden
Threads über die Attribute \lstinline$parties$ und \lstinline$n_waiting$ abgefragt werden.
Das Attribut \lstinline$broken$ gibt an, ob sich die \lstinline$Barrier$ im \glqq zerstörten\grqq{}
Zustand befindet.

\tip{
Wenn in einem Thread ein Problem auftritt, und er nicht ordnugsgemäß fortgesetzt werden kann, kann
\lstinline$abort()$ genutzt werden, um so die bereits wartenden Threads aufzuwecken und einen
Deadlock zu vermeiden.
}

\uebung
\aufgabe{nebenlaufigkeit08}


\subsubsection{Thread-Kommunikation}
\label{threads:subsubsection:thread-kommunikation}

Es gibt verschiedene Wege, eine Kommunikation zwischen verschiedenen Threads zu realisieren.
In Python wurde einer der simpelsten Mechanismen durch die \lstinline$Event$-Klasse implementiert.
\randnotiz{Events}
Hierbei warten Threads darauf, dass ein gewisses Ereignis eintritt, welches durch einen anderen Thread
ausgelöst wird.
Das \lstinline$Event$-Objekt verwaltet hierfür eine interne binäre Variable, welche initial den Wert 
\lstinline$False$ erhält.
Sie kann durch \lstinline$set()$ und \lstinline$clear()$ gesetzt und gelöscht werden.
Die \lstinline$is_set()$-Methode gibt den aktuellen Wert der binären Variable zurück.
Soll auf das Eintreten des Ereignisses gewartet werden, so kann die \lstinline$wait()$-Methode aufgerufen
werden.
Diese blockiert solange, bis \lstinline$set()$ aufgerufen wurde.
Wurde \lstinline$set()$ bereits vor \lstinline$wait()$ aufgerufen, wird nicht blockiert, und \lstinline$wait()$
kehrt sofort zurück.
Es kann wieder der \lstinline$timeout$-Parameter an \lstinline$wait()$ übergeben werden.
Nur falls dieser Timeout ausläuft, bevor \lstinline$set()$ aufgerufen wurde, gibt \lstinline$wait()$ 
\lstinline$False$ zurück.
Andernfalls ist der Rückgabewert immer \lstinline$True$.

\uebung
\aufgabe{nebenlaufigkeit09}

Eine einfache boolesche Variable ist in sehr vielen Anwendungsfällen nicht als Kommunikationsgrundlage
geeignet.
In Python gibt es eine weitere Methode, wie Threads untereinander Informationen sicher austauschen
können.
\randnotiz{Queues}
Im \lstinline$queue$-Modul sind drei Multi-Producer, Multi-Consumer Queues implementiert, welche die
nötigen Synchronisationssemantiken realisieren.
Diese drei Queues unterscheiden sich in der Reihenfolge, in welcher die hinzugefügten Elemente wieder
entnommen werden.
Durch die \lstinline$Queue$-Klasse wird ein FIFO-Queue realsiert und \lstinline$LifoQueue$ handelt nach
dem LIFO- Prinzip.
So kann \lstinline$LifoQueue$ zum Beispiel auch als Stack verwendet werden.
Der \lstinline$PriorityQueue$ hält seine hinzugefügten Elemente sortiert und gibt zuerst immer das
niedrigste Element zurück.
Zur Sortierung wird hierbei der Heap-Queue-Algorithmus, auch bekannt als Priority-Queue-Algorithmus,
aus dem \lstinline$heapq$-Modul (vgl. \cite{pythondokuheapq}) verwendent.
Diese drei Klassen verwenden intern \lstinline$Lock$- Objekte, weshalb Threads bei gleichzeitigen
Zugriffen blockiert werden.
Den Konstruktoren aller Queues kann der optionale Parameter \lstinline$maxsize$ übergeben werden, um 
die maximale Anzahl an Elementen zu setzen.
Wird der Parameter nicht angegeben oder Werte kleiner 1 angegeben, so ist die Größe des Queues 
unbegrenzt.

\kontrollfrage{
\item[\kontroll] Kennen Sie ein Anwendungsbeispiel, bei dem das Konzept von Queues häufig zum
Einsatz kommt?
}

Über die \lstinline$qsize()$-Methode kann die ungefähre Größe der Queues abgefragt werden.
Durch \lstinline$empty()$ und \lstinline$full()$ kann entsprechend geprüft werden, ob der Queue leer
oder gefüllt ist.
Beide geben erwartungsgemäß einen booleschen Wert zurück.
Um ein Element in den Queue einzufügen, wird \lstinline$put()$ aufgerufen. 
Der Parameter \lstinline$item$ nimmt hierbei das Element entgegen.
Ist der Queue bereits voll, blockiert \lstinline$put()$ so lange, bis wieder ein Platz frei geworden ist.
Durch Angabe des optionalen Parameters \lstinline$timeout$ kann eine maximale Zeit angegeben werden,
nach welcher \lstinline$put()$ zurückkehrt, auch wenn kein Platz freigeworden ist.
In diesem Fall würde eine \lstinline$Full$-Exception geworfen werden.
Wird der optionale Parameter \lstinline$block$ auf \lstinline$False$ gesetzt, so kehrt \lstinline$put()$ sofort
zurück und wirft die \lstinline$Full$-Exception, wenn in der Queue kein Platz frei ist.
Es wird auch die Methode \lstinline$put_nowait()$ angeboten. 
Sie verhält sich wie ein Aufruf von \lstinline$put()$, wenn \lstinline$block$ den Wert
\lstinline$False$ besitzt.
Um ein Element aus der Queue heraus zu holen, wird \lstinline$get()$ aufgerufen.
Sollte zum Zeitpunkt des Aufrufs kein Element in der Queue enthalten sein, wird so lange blockiert,
bis ein anderer Thread eines hinzufügt.
Es kann wieder ein optionaler Parameter \lstinline$timeout$ angegeben werden.
Läuft dieser ab, bevor ein Element in den leeren Queue eingefügt wurde, kehrt \lstinline$get()$ zurück
und wirft die \lstinline$Empty$-Exception.
Wird der optionale Paramert \lstinline$block$ mit \lstinline$False$ angegeben, wirft \lstinline$get()$ 
sofort die \lstinline$Empty$-Exception, falls kein Element vorhanden ist.
Das selbe Verhalten tritt auch beim Aufruf der Methode \lstinline$get_nowait()$ auf.
Neben dieser grundlegenden Funktionalität für einen Queue implementieren die drei vorgestellten 
Implementationen noch eine weitere Funktionalität.
So kann geprüft werden, ob alle Elemente vollständig abgearbeitet wurden.
Hierfür werden zwei weitere Methoden angeboten.
Durch den Aufruf von \lstinline$task_done()$ wird den Queues mitgeteilt, dass ein mittels \lstinline$get()$
erhaltenes Element vollständig abgeabreitet wurde.
Wird \lstinline$join()$ auf einem Objekt der drei Queues aufgerufen, so blockiert sie so lange, bis 
\lstinline$task_done()$ so häufig aufgerufen wurde, wie \lstinline$get()$.

\tip{
Diese Funktionalität kann zum Beispiel dazu verwendet werden, um zu warten, bis alle
Dämon-Consumer-Threads ihre Elemente bearbeitet haben, bevor das Programm beendet wird.
}

Neben den bereits vorgestellten Queues gibt es noch eine weitere Implementierung, den
\lstinline$SimpleQueue$.
Er stellt, wie auch \lstinline$Queue$, einen FIFO-Queue dar und bietet zusätzliche Garantien an,
welche auf Kosten von geringerer Funktionalität kommen.
Für einen \lstinline$SimpleQueue$ kann keine maximale Größe definiert werden, er ist somit immer 
unbegrenzt Groß.
Aufrufe von \lstinline$put()$ blockieren garantiert nie.
Die beiden optionalen Parameter \lstinline$block$ und \lstinline$timeout$ werden ignoriert und sind 
nur aufgeführt, um kompatibel zur \lstinline$Queue$-Klasse zu sein.
Die Funktionalität der anderen Klassen, die von \lstinline$SimpleQueue$ nicht unterstützt wird, ist das 
Warten auf die vollständinge Abarbeitung aller Elemente.
Die beiden Methoden \lstinline$task_done()$ und \lstinline$join()$ werden von \lstinline$Simple-$
\lstinline$Queue$ nicht angeboten.

\uebung
\aufgabe{nebenlaufigkeit10}
